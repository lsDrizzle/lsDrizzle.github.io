<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="文献阅读: 3D U-Net"><meta name="keywords" content="Deep Learning,Paper,Computer Vision,Segmentation"><meta name="author" content="lsdrizzle"><meta name="copyright" content="lsdrizzle"><title>文献阅读: 3D U-Net | Drizzle's Blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#paper-information"><span class="toc-number">1.</span> <span class="toc-text"> Paper Information</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#content"><span class="toc-number">2.</span> <span class="toc-text"> Content</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction"><span class="toc-number">2.1.</span> <span class="toc-text"> Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#network-architecture"><span class="toc-number">2.2.</span> <span class="toc-text"> Network Architecture</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#implementation-details"><span class="toc-number">2.3.</span> <span class="toc-text"> Implementation Details</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://avatars3.githubusercontent.com/u/46703360?s=400&amp;u=d146a40aa0586429ad9601fcbc5b187b1bf2b02f&amp;v=4"></div><div class="author-info__name text-center">lsdrizzle</div><div class="author-info__description text-center">御念花菱上，不负少年心。</div><div class="follow-button"><a href="https://github.com/lsDrizzle">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">2</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">4</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">3</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://w.wallhaven.cc/full/2e/wallhaven-2e8ljx.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Drizzle's Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">文献阅读: 3D U-Net</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-07-12</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/科研之路/">科研之路</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/科研之路/文献阅读/">文献阅读</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/科研之路/文献阅读/经典深度学习网络/">经典深度学习网络</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="paper-information"><a class="markdownIt-Anchor" href="#paper-information"></a> Paper Information</h1>
<p>Title: <a href="https://arxiv.org/abs/1606.06650" target="_blank" rel="noopener">3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</a></p>
<p>Authors: <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=%C3%87i%C3%A7ek%2C+%C3%96" target="_blank" rel="noopener">Özgün Çiçek</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Abdulkadir%2C+A" target="_blank" rel="noopener">Ahmed Abdulkadir</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lienkamp%2C+S+S" target="_blank" rel="noopener">Soeren S. Lienkamp</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Brox%2C+T" target="_blank" rel="noopener">Thomas Brox</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ronneberger%2C+O" target="_blank" rel="noopener">Olaf Ronneberger</a></p>
<p>Abstract: A network for volumetric segmentation that learns from sparsely annotated volumetric images, extending the previous u-net to a 3D version.</p>
<h1 id="content"><a class="markdownIt-Anchor" href="#content"></a> Content</h1>
<h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2>
<p>A deep network is proposed to learn to generate dense volumetric segmentations, but only requires some annotated 2D slices for training.</p>
<p>There are two ways to use this network:</p>
<ol>
<li>(Semi-automated) Densification of a sparsely annotated data set</li>
<li>(Fully-automated) Training with multiple sparsely annotated data sets and running on non-annotated data sets to obtain dense segmentation</li>
</ol>
<img src="/2019/07/12/PaperReading_3DUNet/UNetScenario1.png" width="650">
<img src="/2019/07/12/PaperReading_3DUNet/UNetScenario2.png" width="650">
<p>The network is the 3D version of the previous u-net architecture, which consists of a contracting <strong>encoder</strong> to analyze the whole image and an expanding <strong>decoder</strong> to produce a segmentation. The main differences between this network and u-net:</p>
<ol>
<li>Replacing all 2D operations with 3Ds, including 3D conv, 3D max pooling, 3D up-conv layers</li>
<li>Using batch normalization for faster convergence</li>
<li>Avoiding bottlenecks in the architecture</li>
</ol>
<h2 id="network-architecture"><a class="markdownIt-Anchor" href="#network-architecture"></a> Network Architecture</h2>
<p>The network contains an analysis and a synthesis path, each with 4 resolution steps (corresponding to the 4 rows in the figure below).</p>
<img src="/2019/07/12/PaperReading_3DUNet/UNetArch.png" width="700">
<p>In the analysis path, each layer contains two 3x3x3 convolutions, each followed by batch normalization and ReLu, and then a 2x2x2 max pooling with stride=2 in each dimension. In the synthesis path, each layer contains a 2x2x2 up-convolution with stride=2 in each dimension, followed by two 3x3x3 convolutions, each followed by batch normalization and ReLu.</p>
<p>There are shortcut connections (green arrows, using concat) between the two layers (from the analysis and synthesis path, respectively) in the same step (i.e., equal resolution). In the last layer, a 1x1x1 conv reduces the number of output channels to the number of labels (in the paper’s case, it is 3).</p>
<p>In addition, when training, the weights of unlabeled pixels are set to 0 (so-called weighted softmax loss function) to allow the network to train on sparse annotations.</p>
<h2 id="implementation-details"><a class="markdownIt-Anchor" href="#implementation-details"></a> Implementation Details</h2>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">lsdrizzle</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="/lsDrizzle.github.io/2019/07/12/PaperReading_3DUNet/">lsDrizzle.github.io/2019/07/12/PaperReading_3DUNet/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning</a><a class="post-meta__tags" href="/tags/Paper/">Paper</a><a class="post-meta__tags" href="/tags/Computer-Vision/">Computer Vision</a><a class="post-meta__tags" href="/tags/Segmentation/">Segmentation</a></div><nav id="pagination"><div class="next-post pull-right"><a href="/2019/07/11/PaperReading_ResNet/"><span>文献阅读: ResNet</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://w.wallhaven.cc/full/2e/wallhaven-2e8ljx.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 By lsdrizzle</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":130,"height":260,"position":"right","hOffset":0,"vOffset":0},"log":false,"tagMode":false});</script></body></html>